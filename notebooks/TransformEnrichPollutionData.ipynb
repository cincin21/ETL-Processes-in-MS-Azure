{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pollution data transformation\n",
    "\n",
    "In this notebook I transform and enrich sensor data readings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import packages\n",
    "from pyspark.sql.functions import upper, col, desc, explode\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connection strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Azure storage connection settings\n",
    "storage_account_name = \"STORAGE_ACCOUNT_NAME\"\n",
    "storage_account_key = \"STORAGE_ACCOUNT_KEY\"\n",
    "\n",
    "# Azure SQL conneciton settings\n",
    "jdbc_hostname = \"AZURE_SQL_HOSTNAME\"\n",
    "jdbc_port = 1433\n",
    "jdbc_database = \"AZURE_SQL_DATABASE\"\n",
    "jdbc_username = \"AZURE_SQL_USERNAME\"\n",
    "jdbc_password = \"AZURE_SQL_PASSWORD\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create input widgets, which will accept parameters passed in via the ADF Databricks Notebook activity\n",
    "dbutils.widgets.text(\"storage_container_name\", \"\", \"\")\n",
    "\n",
    "# Assign variables to the passed in values of the widgets\n",
    "dbutils.widgets.get(\"storage_container_name\")\n",
    "#storage_container_name = \"sensor-sink-stage\"\n",
    "storage_container_name = getArgument(\"storage_container_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure Storage Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.conf.set(\"fs.azure.account.key.%(storage_account_name)s.blob.core.windows.net\" % locals(), storage_account_key)\n",
    "storage_connection_string = \"wasbs://%(storage_container_name)s@%(storage_account_name)s.blob.core.windows.net\" % locals()\n",
    "\n",
    "# Create data frame\n",
    "pollution_readings_df = spark.read.json(\"%(storage_connection_string)s/*.json\" % locals(), multiLine=True).select(col(\"readingId\").alias(\"ReadingId\"), col(\"locationId\").alias(\"FromLocationId\"), col(\"pollutionLevel\").alias(\"ReadPollutionLevel\"), col(\"readingDateTime\").alias(\"ReadingDateTime\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# URL\n",
    "jdbc_url = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbc_hostname, jdbc_port, jdbc_database)\n",
    "\n",
    "# Connection properties\n",
    "jdbc_connection_properties = {\"user\" : jdbc_username, \"password\" : jdbc_password, \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"}\n",
    "\n",
    "# Define query\n",
    "sensor_locations_query = \"(select Id, City, Country, Population, Latitude, Longitude from SensorLocations) sl\"\n",
    "\n",
    "# Create data frame\n",
    "sensor_locations_df = spark.read.jdbc(url=jdbc_url, table=sensor_locations_query, properties=jdbc_connection_properties).select(col(\"Id\").alias(\"LocationId\"), upper(col(\"City\")).alias(\"City\"), upper(col(\"Country\")).alias(\"Country\"), col(\"Population\"), col(\"Latitude\"), col(\"Longitude\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pollution levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert JSON strings to DataFrames\n",
    "def convertJSONToDataFrame(json, schema=None):\n",
    "  reader = spark.read\n",
    "  if schema:\n",
    "    reader.schema(schema)\n",
    "  return reader.json(sc.parallelize([json]))\n",
    "\n",
    "# Create schema\n",
    "schema = StructType().add(\"values\", MapType(StringType(), StringType()))\n",
    "\n",
    "# Create data frame\n",
    "pollution_level_values = convertJSONToDataFrame(\"\"\"\n",
    "{\n",
    "  \"values\": {\n",
    "    \"1\": \"GOOD\",\n",
    "    \"2\": \"MODERATE\",\n",
    "    \"3\": \"UNHEALTHY\",\n",
    "    \"4\": \"VERY UNHEALTHY\",\n",
    "    \"5\": \"HAZARDOUS\"\n",
    "  }\n",
    "}\n",
    "\"\"\", schema)\n",
    "\n",
    "pollution_level_values = pollution_level_values.select(explode(\"values\").alias(\"PollutionLevelId\", \"PollutionLevel\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join sensor reading data with location and pollution levels data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Join data frames\n",
    "pollution_data_join = pollution_readings_df.join(sensor_locations_df, pollution_readings_df.FromLocationId == sensor_locations_df.LocationId).join(pollution_level_values, pollution_readings_df.ReadPollutionLevel == pollution_level_values.PollutionLevelId).select(\"ReadingId\", \"ReadingDateTime\", \"PollutionLevelId\", \"PollutionLevel\", \"LocationId\", \"City\", \"Country\", \"Population\", \"Latitude\", \"Longitude\").sort(col(\"Population\").desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saved transofrmed data to Azure SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pollution_data_join.write.jdbc(url=jdbc_url,table='SensorReadings',mode='append',properties=jdbc_connection_properties)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "TransformEnrichPollutionData",
  "notebookId": 484406740129719
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
